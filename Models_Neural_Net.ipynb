{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network\n",
    "\n",
    "#### This model attempts to take in limited variables and esitmate future total load\n",
    "\n",
    "- Note: run Models.py for a terminal friendly version of this file and all other models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports:\n",
    "\n",
    "## General imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "\n",
    "## Data preprocessing imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "## Model Evaluation imports\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "## Neural network imports\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import metrics as tfmetrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Neural Network\n",
    "##### Step 1: Data Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import valid csv files\n",
    "Energy = pd.read_csv('./data/energy_dataset.csv')\n",
    "Energy = Energy[Energy['total load actual'].notna()]\n",
    "Weather = pd.read_csv('./EDA/Ameya/AllTemps.csv', skipinitialspace=True)\n",
    "\n",
    "# Get weighted averages of the weather data, as well as integer representation of time\n",
    "pops2017 = {\"V\": 788000, \"M\": 3183000, \"Bi\": 345000, \"Ba\": 1621000, \"S\": 689000}\n",
    "totalPop2017= 788000 + 3183000 + 345000 + 1621000 + 689000\n",
    "pops2017[\"V\"] /= totalPop2017\n",
    "pops2017[\"M\"] /= totalPop2017\n",
    "pops2017[\"Bi\"] /= totalPop2017\n",
    "pops2017[\"Ba\"] /= totalPop2017\n",
    "pops2017[\"S\"] /= totalPop2017\n",
    "def WeightedAverages(Wvar, row, pops):\n",
    "    return (pops[\"V\"] * row[Wvar + \"V\"]) + (pops[\"M\"] * row[Wvar + \"M\"]) + (pops[\"Bi\"] * row[Wvar + \"Bi\"]) + (pops[\"Ba\"] * row[Wvar + \"Ba\"]) + (pops[\"S\"] * row[Wvar + \"S\"])\n",
    "def IsolateHour(Time):\n",
    "    Time = Time.split(\" \")\n",
    "    Time = Time[1].split(\":\")\n",
    "    return Time[0]\n",
    "IntTimes = []\n",
    "TempsAverage = []\n",
    "HumAvererage = []\n",
    "CldAverage = []\n",
    "WndAverage = []\n",
    "for index, row in Weather.iterrows():\n",
    "    TempsAverage.append(WeightedAverages(\"temp\", row, pops2017))\n",
    "    HumAvererage.append(WeightedAverages(\"hum\", row, pops2017))\n",
    "    CldAverage.append(WeightedAverages(\"cld\", row, pops2017))\n",
    "    WndAverage.append(WeightedAverages(\"wnd\", row, pops2017))\n",
    "    IntTimes.append(IsolateHour(row[\"time\"]))\n",
    "WxAverages = pd.DataFrame({\n",
    "    \"time\": np.asarray(IntTimes).astype('float64'),\n",
    "    \"tAve\": np.asarray(TempsAverage).astype('float64'),\n",
    "    \"hAve\": np.asarray(HumAvererage).astype('float64'),\n",
    "    \"cAve\": np.asarray(CldAverage).astype('float64'),\n",
    "    \"wAve\": np.asarray(WndAverage).astype('float64')\n",
    "})\n",
    "\n",
    "# Scale the data with the MinMax method\n",
    "colScales = []\n",
    "colMins = []\n",
    "def MinMaxScaler(column):\n",
    "    scale = max(column)-min(column)\n",
    "    colScales.append(scale)\n",
    "    colMins.append(min(column))\n",
    "    return (column-min(column))/scale   \n",
    "for col in WxAverages.columns:\n",
    "    WxAverages[col] =   MinMaxScaler(WxAverages[col])\n",
    "load = MinMaxScaler(Energy['total load actual'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 2: Create and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create model\n",
    "def create_model(h1=3, h2=3):\n",
    "    model = Sequential()\n",
    "    layers = [\n",
    "        Dense(5, activation='elu', name='input', kernel_initializer=\"normal\"),\n",
    "        Dense(h1, activation='elu', name='h1', kernel_initializer=\"normal\"),\n",
    "        Dense(h2, activation='elu', name='h2', kernel_initializer=\"normal\"),\n",
    "        Dense(1, activation='tanh', name='out', kernel_initializer=\"normal\")\n",
    "    ]\n",
    "    for layer in layers:\n",
    "        model.add(layer)\n",
    "    model.compile(loss='mean_squared_error', optimizer=Adam(learning_rate=0.001), metrics=[tfmetrics.RootMeanSquaredError()])\n",
    "    return model\n",
    "\n",
    "# Create the model\n",
    "loadNN = create_model()\n",
    "\n",
    "# Create testing and training dataset\n",
    "wxtrain, wxtest, loadtrain, loadtest = train_test_split(WxAverages, load, test_size = 0.1, random_state=0)\n",
    "\n",
    "# Train the model\n",
    "hist = loadNN.fit(wxtrain.to_numpy(), loadtrain.to_numpy(), epochs=500, verbose=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 3: Evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load Neural Network Scores:\n",
      "Testing MSE: 0.01849\n",
      "Testing RMSE: 0.13596\n",
      "r^2 score: 0.53599\n"
     ]
    }
   ],
   "source": [
    "# Get evaluation scores\n",
    "load_pred = loadNN.predict(wxtest, verbose=0)\n",
    "MSE = mean_squared_error(y_true=loadtest, y_pred=load_pred)\n",
    "RMSE = mean_squared_error(y_true=loadtest, y_pred=load_pred, squared=False)\n",
    "r2 = r2_score(y_true=loadtest, y_pred=load_pred)\n",
    "print(\"Load Neural Network Scores:\")\n",
    "print('Testing MSE: %.5f' % MSE)\n",
    "print('Testing RMSE: %.5f' % RMSE)\n",
    "print(\"r^2 score: %.5f\" % r2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Step 4: Save the model\n",
    "- run this cell to save the model that has been trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ./savedmodels/LoadNN.ann/assets\n"
     ]
    }
   ],
   "source": [
    "# Save the model to disk\n",
    "loadNN.save('./savedmodels/LoadNN.ann')"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
  },
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
